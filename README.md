# Transliteration with Neural Sequence to Sequence Networks

## About
This repository contains my paper "Transliteration with Neural Sequence to Sequence Networks" that I wrote in 2017 as student at LMU Munich inthe master program "Computational Linguistics". 

## Overview
In the paper I analyze the performance of different sequence to sequence architectures (seq2seq) for a simple transliteration task. The task is 
inspired by the german dialect franconian with some phonetic rules. 
The dialect is simulated by artificially generating a training set of words and their transliteration based on a simple grammer.
The paper analyzes the performance of different seq2seq models.
It also analyzes the attention patterns that correspond to the different rules of the grammar.

Examples:

Peter -> BÃ¤da\
gelaufen -> glafa\
gelungen -> glunga

## The paper
Link: [TranslitSeq2SeqAttention.pdf](doc/TranslitSeq2SeqAttention.pdf)